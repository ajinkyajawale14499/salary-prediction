# -*- coding: utf-8 -*-
"""Salary_Prediction_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Z3Dh4DbvsYid2bPJRiS-haslgJaaWXkM
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# import os
# for dirname, _, filenames in os.walk('/data'):
#     for filename in filenames:
#         print(os.path.join(dirname, filename))

df_train_salaries = pd.read_csv('/train_salaries.csv', header=0, sep=',', quotechar='"')
df_train_salaries.tail()

df_train_features = pd.read_csv('/train_features.csv', header=0, sep=',', quotechar='"')
df_train_features.tail()

df_test_features = pd.read_csv('/test_features.csv', header=0, sep=',', quotechar='"')
df_test_features.tail()

"""### EDA"""

df_train_salaries.describe()

"""### Understanding data"""

print(" \n Train salaries \n")

df_train_salaries.tail().info()

print(" \n Train features \n")

df_train_features.tail().info()

print(" \n Test features \n")

df_test_features.tail().info()

"""### Null Values Handling"""

print(" \n Train features \n")

print(" in train dataset \n\n",df_train_features.isna().sum())


print(" \n Test features \n")

print("Null Values in test dataset \n\n",df_test_features.isna().sum())

print(" \n Train salaries \n")

print("Null Values in train salaries dataset \n\n",df_train_salaries.isna().sum())

print(df_train_features.head(5))
print(len(df_train_features))

print(df_train_salaries.head(5))
print(len(df_train_salaries))

print(df_test_features.head(5))
print(len(df_test_features))

#combined the features and salaries in the training data
training_df = pd.merge(df_train_features,df_train_salaries, how = 'inner', on = 'jobId')
print(training_df.head(5))
print(len(training_df))

#look for duplicated data and invalid data
training_df = training_df.drop_duplicates(subset="jobId")

print(len(training_df))

test_df = df_test_features.drop_duplicates(subset="jobId")
print(len(test_df))

# summarize
training_df.info()
training_df.describe(include = 'all')

"""### feature engineering

"""

# min salary is 0, removing unnecessary rows
print(training_df.loc[training_df["salary"]==0])

def convert_to_category(df, col):
    df[col] = df[col].astype('category')
    return df

# converting features into categorical

training_df = convert_to_category(training_df, 'companyId')
training_df = convert_to_category(training_df, 'jobType')
training_df = convert_to_category(training_df, 'degree')
training_df = convert_to_category(training_df, 'major')
training_df = convert_to_category(training_df, 'industry')
training_df.info()

# salary Normal distribution

plt.figure(figsize = (14, 6))
plt.subplot(1, 2, 1)
sns.boxplot(training_df.salary)
plt.subplot(1, 2, 2)
sns.distplot(training_df.salary, bins = 20)
plt.show()

# if value_counts < 20 or if the variable is not numeric, print the value_count table

# if the variable is numeric, make density plot for frequency, and lineplot for interaction with the target variable

# if the variable is categorical and number of category is less than 20,

def feature_plot(df, target, col):
    categories = df[col].value_counts().index.to_list()
    n_categories = len(categories)
    if n_categories < 20 or df[col].dtype != 'int64':
        print(df[col].value_counts())

    plt.figure(figsize = (14,10))
    if df[col].dtype == 'int64':
        plt.subplot(2,1,1)
        if n_categories < 30:
            sns.distplot(df[col], bins = n_categories)
        else:
            sns.distplot(df[col], bins = 20)
        plt.subplot(2,1,2)
        sns.lineplot(x = col, y = target, data = df)
    else:
        if n_categories < 20:
            plt.subplot(2,1,1)
            sns.countplot(x = col, data = df)
            plt.subplot(2,1,2)
            sns.boxplot(x = col, y = target, data = df)
    plt.show()

feature_plot(training_df, 'salary','jobType')

feature_plot(training_df, 'salary','major')

feature_plot(training_df, 'salary','industry')

feature_plot(training_df, 'salary','degree')

feature_plot(training_df, 'salary','yearsExperience')

feature_plot(training_df,'salary','milesFromMetropolis')

"""### handling outliers"""

def outliers(df, col):
    stat = df[col].describe()
    IQR = stat['75%'] - stat['25%']
    upper = stat['75%'] + 1.5 * IQR
    lower = stat['25%'] - 1.5 * IQR
    print('The upper and lower bounds for variable {} are {} and {}'.format(col, upper, lower))
    return lower, upper

lower_boundries, upper_boundries = outliers(training_df, 'salary')

# outliers above lower bounds
upper_outliers = training_df[training_df.salary > upper_boundries]
print(upper_outliers.shape)
upper_outliers.jobType.value_counts()

upper_outliers[upper_outliers['jobType'] == 'JUNIOR']

"""## Preprocessing"""

from sklearn.preprocessing import LabelEncoder

def categorical_encoding(training, test):
    from sklearn import preprocessing
    cols = training.select_dtypes(include=['category']).columns.to_list()
    for col in cols:
        le = preprocessing.LabelEncoder()
        le.fit(training[col])
        training[col+'_encoded'] = le.transform(training[col])
        test[col+'_encoded'] = le.transform(test[col])
    return training, test

from sklearn.preprocessing import LabelEncoder

def label_encode(df, column, target_column):
    le = LabelEncoder()
    df[target_column] = le.fit_transform(df[column])
    return df

import pandas as pd
import seaborn as sns
from sklearn.preprocessing import LabelEncoder

# Assuming you have a DataFrame called 'data' with columns: jobType, degree, major, industry, yearsExperience, milesFromMetropolis, and salary

# Encode categorical variables with the mean of the salary for each category
def encode_categorical_with_mean(df, cat_column, target_column):
    le = LabelEncoder()
    df[cat_column + '_encoded'] = le.fit_transform(df[cat_column])
    category_mean_salary = df.groupby(cat_column)[target_column].mean().to_dict()
    df[cat_column + '_encoded'] = df[cat_column].map(category_mean_salary)
    return df

categorical_columns = ['jobType', 'degree', 'major', 'industry']

for column in categorical_columns:
    data = encode_categorical_with_mean(training_df, column, 'salary')

# Plot feature correlation with salary
correlation_matrix = data.corr()
sns.heatmap(correlation_matrix[['salary']], annot=True, cmap='coolwarm', vmin=-1, vmax=1)

from sklearn.preprocessing import LabelEncoder

def label_encode(df, column, target_column):
    le = LabelEncoder()
    df[target_column] = le.fit_transform(df[column])
    return df

# Example usage
training_test = label_encode(training_copy, 'major', 'salary')

training_test.info()

training_copy = training_df[:]
for col in training_copy.columns:
    if training_copy[col].dtype.name == 'category':
        # training_copy[col]=label_encode(training_copy, col, 'salary')
        # training_copy[col] = training_copy[col].astype('int64')
        print(training_copy[col])

# training_copy.head()
# training_copy.info()

fig = plt.figure(figsize = (12,10))
features = ['companyId', 'jobType', 'degree', 'major', 'industry', 'yearsExperience', 'milesFromMetropolis']
sns.heatmap(training_copy[features + ['salary']].corr(), cmap = 'BuPu', annot=True)
plt.xticks(rotation = 45)
plt.show()

import pandas as pd
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import mean_squared_error
from sklearn.base import BaseEstimator, TransformerMixin

# Assuming you have a DataFrame called 'data' with columns: industry, salary

# Custom Transformer to calculate mean salary for each industry
class MeanSalaryTransformer(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.mean_salaries = None

    def fit(self, X, y=None):
        self.mean_salaries = X.groupby('industry')['salary'].mean().to_dict()
        return self

    def transform(self, X):
        X_copy = X.copy()
        X_copy['mean_salary'] = X_copy['industry'].map(self.mean_salaries)
        return X_copy[['mean_salary']]

    def predict(self, X):
        # Since this is a simple model, we can just return the mean_salary values
        return X['mean_salary']

# Initialize the MeanSalaryTransformer
mean_salary_transformer = MeanSalaryTransformer()

# Apply 5-fold cross-validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Create a simple model pipeline
from sklearn.pipeline import make_pipeline

simple_model = make_pipeline(mean_salary_transformer)

# Use negative RMSE as cross_val_score expects a scoring function where higher is better
rmse_scores = cross_val_score(simple_model, data, data['salary'], cv=kf, scoring='neg_mean_squared_error')

# Calculate RMSE
rmse_scores = pd.Series(-rmse_scores)
rmse = rmse_scores.apply(lambda x: (abs(x))**0.5)

print("Root Mean Squared Error (RMSE) for the simple model:", rmse.mean())

def salary_get_variable_list():
    variable_list = ['companyId', 'jobType', 'degree', 'major', 'industry']
    return variable_list

#transform a variable based on the average of the target variable of each value
#for example, transfor each industry into the averaged salary of each industry
def transform_categorical(df, col, target, training_df):
    category_mean = {}
    value_list = df[col].cat.categories.tolist()
    for value in value_list:
        category_mean[value] = training_df[training_df[col] == value][target].mean()
    df[col+'_transformed'] = df[col].map(category_mean)
    df[col+'_transformed'] = df[col+'_transformed'].astype('int64')
    return df


#make sure training and test has the same categorical variables
def encode_categorical(training, test):
    from sklearn import preprocessing
    cols = training.select_dtypes(include=['category']).columns.to_list()
    for col in cols:
        le = preprocessing.LabelEncoder()
        le.fit(training[col])
        training[col+'_encoded'] = le.transform(training[col])
        test[col+'_encoded'] = le.transform(test[col])
    return training, test

def convert_to_category(df, col):
    df[col] = df[col].astype('category')
    return df

def drop_duplicates(df, col):
    df = df.drop_duplicates(subset = col)
    return df

def salary_preprocess():
    #define constants
    variable_list = salary_get_variable_list()
    target = 'salary'

    #read the data and merge feature and salary for the training data
    features = pd.read_csv('/train_features.csv')
    salaries = pd.read_csv('/train_salaries.csv')
    test = pd.read_csv('/test_features.csv')
    training = pd.merge(features,salaries, how = 'inner', on = 'jobId')

    #remove duplicates
    training = drop_duplicates(training,'jobId')
    test = drop_duplicates(test,'jobId')

    #remove salary = 0 in the training set
    training = training.drop(training[training[target]==0].index)

    #convert object to categorial variables
    #and transform them based on mean target (salary)
    for variable in variable_list:
        training = convert_to_category(training, variable)
        training = transform_categorical(training, variable, target, training)
        test = convert_to_category(test, variable)
        test = transform_categorical(test, variable, target, training)

    #encode categorical variables to dummies
    training, test = encode_categorical(training, test)

    #save scaler for later

    #print results on the screen
    training.info()
    training.head()
    test.info()
    test.head()
    return training, test


training, test = salary_preprocess()

training.head()

test.head()

"""## Baseline models

1. Calculating Mean Squared Error (MSE) in the training dataset

- use the average salary of each industry
- use the average salary of each major
- use the average salary of each degree
- use the averaged salary of each jobType
"""

#the variable 'industry_transformed' had transformed each indusstry with the mean salary of that industry
#thus it can server as our prediction, same with 'major_transformed', 'degree_transformed', 'jobType_transformed'
prediction_by_industry = mean_squared_error(training['salary'], training['industry_transformed'])
print('Prediction by industry is {}'.format(prediction_by_industry))
prediction_by_major = mean_squared_error(training['salary'], training['major_transformed'])
print('Prediction by major is {}'.format(prediction_by_major))
prediction_by_degree = mean_squared_error(training['salary'], training['degree_transformed'])
print('Prediction by degree is {}'.format(prediction_by_degree))
prediction_by_jobType = mean_squared_error(training['salary'], training['jobType_transformed'])
print('Prediction by jobType is {}'.format(prediction_by_jobType))

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import cross_val_score
from sklearn.utils import shuffle
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LinearRegression

models = []
mean_mse = {}
cv_std = {}
res = {}
n_procs = 4

lr = LinearRegression()
rf = RandomForestRegressor(n_estimators = 80, n_jobs = n_procs, max_depth = 20, min_samples_split = 70,
                          max_features = 7, verbose = 0)
gbm = GradientBoostingRegressor(n_estimators = 40, max_depth = 7, loss = 'ls', verbose = 0)

models.extend([lr, rf, gbm])

feature_transformed = ['yearsExperience', 'milesFromMetropolis', 'companyId_transformed',
                'jobType_transformed', 'degree_transformed', 'major_transformed', 'industry_transformed']
feature_encoded = ['yearsExperience', 'milesFromMetropolis', 'companyId_transformed',
                'jobType_encoded', 'degree_encoded', 'major_encoded', 'industry_encoded']

from sklearn.model_selection import cross_val_score
def cross_val_model(model, feature_df, target, n_procs, mean_mse, cv_std):
    neg_mse = cross_val_score(model, feature_df, target, cv = 5, n_jobs = n_procs,
                              scoring = 'neg_mean_squared_error')
    mean_mse[model] = -1.0 * np.mean(neg_mse)
    cv_std[model] = np.std(neg_mse)


#print a short summary
def print_summary(model, mean_mse, cv_std):
    print('\nmodel:\n', model)
    print('Average MSE:\n', mean_mse[model])
    print('Standard deviation during cross validation:\n', cv_std[model])


#feature importance
def get_model_feature_importances(model, feature_df):
    if hasattr(model, 'feature_importances_'):
        importances = model.feature_importances_
    else:
        importances = [0] * len(feature_df.columns)

    feature_importances = pd.DataFrame({'feature': feature_df.columns, 'importance': importances})
    feature_importances.sort_values(by = 'importance', ascending = False, inplace = True)
    ''' set the index to 'feature' '''
    feature_importances.set_index('feature', inplace = True, drop = True)
    return feature_importances

# #  Cross validation - feature transformed not scaled
# for model in models:
#     cross_val_model(model, training[feature_transformed], training['salary'], n_procs, mean_mse, cv_std)
#     print_summary(model, mean_mse, cv_std)

# feature_encoded & not scaled
for model in models:
    cross_val_model(model, training[feature_encoded], training['salary'], n_procs, mean_mse, cv_std)
    print_summary(model, mean_mse, cv_std)

#scale numeric variables
#weight to see whether to run it or not
training, test = apply_scaler(training, test, 'salary')

# feature transformed & scaled
for model in models:
    cross_val_model(model, training[feature_cols], training['salary'], n_procs, mean_mse, cv_std)
    print_summary(model, mean_mse, cv_std)

# feature encoded scaled
for model in models:
    cross_val_model(model, training[feature_encoded], training['salary'], n_procs, mean_mse, cv_std)
    print_summary(model, mean_mse, cv_std)

# Best model based on MSE - transformed and scaled features using Gradient boosting
bestModel = GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
                          learning_rate=0.1, loss='ls', max_depth=7,
                          max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=1, min_samples_split=2,
                          min_weight_fraction_leaf=0.0, n_estimators=40,
                          n_iter_no_change=None, presort='auto',
                          random_state=None, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)

"""### Training"""

bestModel.fit(training[feature_transformed], training['salary'])

GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
                          learning_rate=0.1, loss='ls', max_depth=7,
                          max_features=None, max_leaf_nodes=None,
                          min_impurity_decrease=0.0, min_impurity_split=None,
                          min_samples_leaf=1, min_samples_split=2,
                          min_weight_fraction_leaf=0.0, n_estimators=40,
                          n_iter_no_change=None, presort='auto',
                          random_state=None, subsample=1.0, tol=0.0001,
                          validation_fraction=0.1, verbose=0, warm_start=False)

"""### Model Testing"""

predicted_salaries = bestModel.predict(test[feature_transformed])

test['predicted_salary'] = predicted_salaries.tolist()

test.head(10)

test.to_csv('test_salary.csv', index = False)

"""### feature importance"""

feature_importances = get_model_feature_importances(bestModel, training[feature_transformed])

feature_importances.plot.bar(figsize=(20,10))

